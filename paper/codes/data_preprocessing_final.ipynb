{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear IPython's global namespace\n",
    "%reset -f\n",
    "\n",
    "# Reimport gc module\n",
    "import gc\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear all cell outputs\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/manimala/Documents/satyakama/paper-farmer-chatbot/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl \n",
    "pl.Config.set_tbl_rows(1000)  # or whatever number of rows you want to see\n",
    "pl.Config.set_tbl_cols(-1)  # Show all columns (-1 means no limit)\n",
    "pl.Config.set_fmt_str_lengths(1000)  # Increase maximum string length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting row count: 41987874\n",
      "Rows removed after initial filtering: 21539071\n",
      "Rows removed after digit filtering: 3267964\n",
      "Rows removed after numeric pattern filtering: 395171\n",
      "Final row count: 16785668\n"
     ]
    }
   ],
   "source": [
    "def preprocess_kcc_dataset(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the KCC dataset by cleaning and transforming the data.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the KCC dataset CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pl.DataFrame: Cleaned and preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read CSV with selected columns\n",
    "    master_df = pl.read_csv(\n",
    "        source=file_path,\n",
    "        columns=[\n",
    "            'Year', 'Month', 'Day', 'Crop', 'BlockName', \n",
    "            'DistrictName', 'QueryType', 'Season', 'Sector',\n",
    "            'StateName', 'QueryText', 'KccAns'\n",
    "        ],\n",
    "        has_header=True,\n",
    "        low_memory=True\n",
    "    )\n",
    "\n",
    "    print(f\"Starting row count: {len(master_df)}\")\n",
    "\n",
    "    # First clean QueryText and KccAns\n",
    "    master_df = master_df.with_columns([\n",
    "        pl.col(\"QueryText\").str.replace_all(r\"\\s+\", \" \").str.strip_chars().alias(\"QueryText\"),\n",
    "        pl.col(\"KccAns\").str.replace_all(r\"\\s+\", \" \").str.strip_chars().alias(\"KccAns\")\n",
    "    ])\n",
    "\n",
    "    # Then convert all to uppercase\n",
    "    master_df = master_df.with_columns([\n",
    "        pl.all().cast(pl.Utf8).str.to_uppercase()\n",
    "    ])\n",
    "\n",
    "    # Create Date column and drop date-related columns\n",
    "    master_df = master_df.with_columns([\n",
    "        pl.format(\"{}-{}-{}\",\n",
    "            pl.col(\"Day\").cast(pl.Utf8).str.zfill(2),\n",
    "            pl.col(\"Month\").cast(pl.Utf8).str.zfill(2),\n",
    "            pl.col(\"Year\")\n",
    "        ).str.strptime(pl.Date, format=\"%d-%m-%Y\").alias(\"Date\")\n",
    "    ]).drop(['Day', 'Month', 'Year'])\n",
    "\n",
    "    # Combine multiple filtering conditions\n",
    "    initial_count = len(master_df)\n",
    "    master_df = master_df.filter(\n",
    "        # Basic text validations\n",
    "        pl.col(\"QueryText\").str.contains(r\"[a-zA-Z0-9]\") & \n",
    "        pl.col(\"KccAns\").str.contains(r\"[a-zA-Z0-9]\") &\n",
    "        # Remove specific QueryText values\n",
    "        (pl.col(\"QueryType\") != \"WEATHER\") &\n",
    "        (pl.col(\"QueryText\") != \"TEST CALL\") &\n",
    "        (pl.col(\"QueryText\") != \"BLANK CALL\") &\n",
    "        ~pl.col(\"QueryText\").str.contains(\"WEATHER\") &\n",
    "        # Remove whitespace-only entries\n",
    "        ~pl.col(\"QueryText\").str.contains(r\"^\\s*$\") & \n",
    "        ~pl.col(\"KccAns\").str.contains(r\"^\\s*$\")\n",
    "    )\n",
    "    print(f\"Rows removed after initial filtering: {initial_count - len(master_df)}\")\n",
    "\n",
    "    # Remove rows with digits in specific columns\n",
    "    initial_count = len(master_df)\n",
    "    for col in ['BlockName', 'Crop', 'QueryType', 'Sector']:\n",
    "        master_df = master_df.filter(\n",
    "            ~pl.col(col).str.contains(r\"\\d\")\n",
    "        )\n",
    "    print(f\"Rows removed after digit filtering: {initial_count - len(master_df)}\")\n",
    "\n",
    "    # Remove numeric-only entries\n",
    "    initial_count = len(master_df)\n",
    "    numeric_pattern = r\"^[-]?[0-9]*\\.?[0-9]+$\"\n",
    "    master_df = master_df.filter(\n",
    "        ~pl.col(\"QueryText\").str.contains(numeric_pattern) &\n",
    "        ~pl.col(\"KccAns\").str.contains(numeric_pattern)\n",
    "    )\n",
    "    print(f\"Rows removed after numeric pattern filtering: {initial_count - len(master_df)}\")\n",
    "\n",
    "    # Handle Season column and null values\n",
    "    master_df = master_df.with_columns([\n",
    "        pl.when(pl.col(\"Season\").is_null() | (pl.col(\"Season\") == \"0\"))\n",
    "        .then(pl.lit(\"UNSPECIFIED\"))\n",
    "        .otherwise(pl.col(\"Season\"))\n",
    "        .alias(\"Season\")\n",
    "    ]).drop_nulls()\n",
    "\n",
    "    print(f\"Final row count: {len(master_df)}\")\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "# Usage example:\n",
    "filtered_df = preprocess_kcc_dataset('dataset/original_dataset/kcc_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>BlockName</th><th>Crop</th><th>DistrictName</th><th>QueryType</th><th>Season</th><th>Sector</th><th>StateName</th><th>QueryText</th><th>KccAns</th><th>Date</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>date</td></tr></thead><tbody><tr><td>&quot;MOHANPUR&quot;</td><td>&quot;COCONUT&quot;</td><td>&quot;SAMASTIPUR&quot;</td><td>&quot;FERTILIZER USE AND AVAILABILITY&quot;</td><td>&quot;KHARIF&quot;</td><td>&quot;HORTICULTURE&quot;</td><td>&quot;BIHAR&quot;</td><td>&quot;FERTILIZER DOSES OF COCONUT&quot;</td><td>&quot;FERTILIZER ARE NPK 1:2:2 KGPLANT&quot;</td><td>2007-01-05</td></tr><tr><td>&quot;DOLONGGHAT&quot;</td><td>&quot;BANANA&quot;</td><td>&quot;NAGAON&quot;</td><td>&quot;FERTILIZER USE AND AVAILABILITY&quot;</td><td>&quot;JAYAD&quot;</td><td>&quot;HORTICULTURE&quot;</td><td>&quot;ASSAM&quot;</td><td>&quot;ASKING ABOUT THE FERTILIZER SCHEDULE FOR BANANA CULTIVATION&quot;</td><td>&quot;SUGGESTED TO APPLY UREA242GRAMPLANTSSP206GRAMPLANTMOP551GRAMPLANT AND COMPOST12KGPLANT IN TRENCH METHOD&quot;</td><td>2009-09-29</td></tr><tr><td>&quot;DANIYAWAN&quot;</td><td>&quot;WHEAT&quot;</td><td>&quot;PATNA&quot;</td><td>&quot;FERTILIZER USE AND AVAILABILITY&quot;</td><td>&quot;KHARIF&quot;</td><td>&quot;AGRICULTURE&quot;</td><td>&quot;BIHAR&quot;</td><td>&quot;ASKING ABOUT FERTILISER DOSE OF WHEAT&quot;</td><td>&quot;ASKING ABOUT FERTILISER DOSE OF WHEAT ARE 120KG N60KG P40KG KHECT FOR SOWING STAGE&quot;</td><td>2009-12-23</td></tr><tr><td>&quot;AKHORIGOLA&quot;</td><td>&quot;CABBAGE&quot;</td><td>&quot;ROHTAS&quot;</td><td>&quot;CULTURAL PRACTICES&quot;</td><td>&quot;KHARIF&quot;</td><td>&quot;HORTICULTURE&quot;</td><td>&quot;BIHAR&quot;</td><td>&quot;EARLY CULTIVAR OF CABBAGE&quot;</td><td>&quot;PUSA DRUM HEAD&quot;</td><td>2009-02-22</td></tr><tr><td>&quot;HATHUA&quot;</td><td>&quot;GLADIOLUS&quot;</td><td>&quot;GOPALGANJ&quot;</td><td>&quot;CULTURAL PRACTICES&quot;</td><td>&quot;RABI&quot;</td><td>&quot;HORTICULTURE&quot;</td><td>&quot;BIHAR&quot;</td><td>&quot;METHOD OF GLADIOLUS CULTIVATION&quot;</td><td>&quot;ANSWER GIVEN IN DETAILS&quot;</td><td>2009-05-28</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌─────────┬─────────┬─────────┬─────────┬────────┬─────────┬─────────┬─────────┬─────────┬─────────┐\n",
       "│ BlockNa ┆ Crop    ┆ Distric ┆ QueryTy ┆ Season ┆ Sector  ┆ StateNa ┆ QueryTe ┆ KccAns  ┆ Date    │\n",
       "│ me      ┆ ---     ┆ tName   ┆ pe      ┆ ---    ┆ ---     ┆ me      ┆ xt      ┆ ---     ┆ ---     │\n",
       "│ ---     ┆ str     ┆ ---     ┆ ---     ┆ str    ┆ str     ┆ ---     ┆ ---     ┆ str     ┆ date    │\n",
       "│ str     ┆         ┆ str     ┆ str     ┆        ┆         ┆ str     ┆ str     ┆         ┆         │\n",
       "╞═════════╪═════════╪═════════╪═════════╪════════╪═════════╪═════════╪═════════╪═════════╪═════════╡\n",
       "│ MOHANPU ┆ COCONUT ┆ SAMASTI ┆ FERTILI ┆ KHARIF ┆ HORTICU ┆ BIHAR   ┆ FERTILI ┆ FERTILI ┆ 2007-01 │\n",
       "│ R       ┆         ┆ PUR     ┆ ZER USE ┆        ┆ LTURE   ┆         ┆ ZER     ┆ ZER ARE ┆ -05     │\n",
       "│         ┆         ┆         ┆ AND AVA ┆        ┆         ┆         ┆ DOSES   ┆ NPK     ┆         │\n",
       "│         ┆         ┆         ┆ ILABILI ┆        ┆         ┆         ┆ OF      ┆ 1:2:2   ┆         │\n",
       "│         ┆         ┆         ┆ TY      ┆        ┆         ┆         ┆ COCONUT ┆ KGPLANT ┆         │\n",
       "│ DOLONGG ┆ BANANA  ┆ NAGAON  ┆ FERTILI ┆ JAYAD  ┆ HORTICU ┆ ASSAM   ┆ ASKING  ┆ SUGGEST ┆ 2009-09 │\n",
       "│ HAT     ┆         ┆         ┆ ZER USE ┆        ┆ LTURE   ┆         ┆ ABOUT   ┆ ED TO   ┆ -29     │\n",
       "│         ┆         ┆         ┆ AND AVA ┆        ┆         ┆         ┆ THE FER ┆ APPLY   ┆         │\n",
       "│         ┆         ┆         ┆ ILABILI ┆        ┆         ┆         ┆ TILIZER ┆ UREA242 ┆         │\n",
       "│         ┆         ┆         ┆ TY      ┆        ┆         ┆         ┆ SCHEDUL ┆ GRAMPLA ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆ E FOR   ┆ NTSSP20 ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆ BANANA  ┆ 6GRAMPL ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆ CULTIVA ┆ ANTMOP5 ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆ TION    ┆ 51GRAMP ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ LANT    ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ AND COM ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ POST12K ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ GPLANT  ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ IN      ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ TRENCH  ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ METHOD  ┆         │\n",
       "│ DANIYAW ┆ WHEAT   ┆ PATNA   ┆ FERTILI ┆ KHARIF ┆ AGRICUL ┆ BIHAR   ┆ ASKING  ┆ ASKING  ┆ 2009-12 │\n",
       "│ AN      ┆         ┆         ┆ ZER USE ┆        ┆ TURE    ┆         ┆ ABOUT   ┆ ABOUT   ┆ -23     │\n",
       "│         ┆         ┆         ┆ AND AVA ┆        ┆         ┆         ┆ FERTILI ┆ FERTILI ┆         │\n",
       "│         ┆         ┆         ┆ ILABILI ┆        ┆         ┆         ┆ SER     ┆ SER     ┆         │\n",
       "│         ┆         ┆         ┆ TY      ┆        ┆         ┆         ┆ DOSE OF ┆ DOSE OF ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆ WHEAT   ┆ WHEAT   ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ ARE     ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ 120KG   ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ N60KG   ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ P40KG   ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ KHECT   ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ FOR     ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ SOWING  ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆         ┆ STAGE   ┆         │\n",
       "│ AKHORIG ┆ CABBAGE ┆ ROHTAS  ┆ CULTURA ┆ KHARIF ┆ HORTICU ┆ BIHAR   ┆ EARLY   ┆ PUSA    ┆ 2009-02 │\n",
       "│ OLA     ┆         ┆         ┆ L PRACT ┆        ┆ LTURE   ┆         ┆ CULTIVA ┆ DRUM    ┆ -22     │\n",
       "│         ┆         ┆         ┆ ICES    ┆        ┆         ┆         ┆ R OF    ┆ HEAD    ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆ CABBAGE ┆         ┆         │\n",
       "│ HATHUA  ┆ GLADIOL ┆ GOPALGA ┆ CULTURA ┆ RABI   ┆ HORTICU ┆ BIHAR   ┆ METHOD  ┆ ANSWER  ┆ 2009-05 │\n",
       "│         ┆ US      ┆ NJ      ┆ L PRACT ┆        ┆ LTURE   ┆         ┆ OF GLAD ┆ GIVEN   ┆ -28     │\n",
       "│         ┆         ┆         ┆ ICES    ┆        ┆         ┆         ┆ IOLUS   ┆ IN      ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆ CULTIVA ┆ DETAILS ┆         │\n",
       "│         ┆         ┆         ┆         ┆        ┆         ┆         ┆ TION    ┆         ┆         │\n",
       "└─────────┴─────────┴─────────┴─────────┴────────┴─────────┴─────────┴─────────┴─────────┴─────────┘"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts = (\n",
    "#     filtered_df.get_column(\"QueryText\")\n",
    "#     .value_counts(parallel=True)\n",
    "#     .with_columns([\n",
    "#         (pl.col(\"count\") / pl.col(\"count\").sum() * 100).alias(\"percentage\")  # Note: \"count\" not \"counts\"\n",
    "#     ])\n",
    "#     .sort(\"count\", descending=True) \n",
    "#     .head(500)\n",
    "# )\n",
    "\n",
    "# print(value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from typing import List, Dict, Union\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, \n",
    "                 model_name: str = 'BAAI/bge-large-en-v1.5',\n",
    "                 device: str = 'cuda',\n",
    "                 batch_size: int = 128):\n",
    "        \"\"\"\n",
    "        Initialize the DataProcessor with a specified text encoder model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the sentence-transformer model to use\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize text encoder\n",
    "        self.logger.info(f\"Loading text encoder model: {model_name}\")\n",
    "        try:\n",
    "            self.text_encoder = SentenceTransformer(model_name)\n",
    "            self.text_encoder.to(device)\n",
    "            self.device = device\n",
    "            self.batch_size = batch_size\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading text encoder: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "        # Initialize state variables\n",
    "        self.categorical_columns = ['Crop', 'DistrictName', 'QueryType', \n",
    "                                  'Season', 'Sector', 'StateName']\n",
    "        self.text_columns = ['QueryText', 'KccAns']\n",
    "        self.processed_df = None\n",
    "        self.embeddings_cache = {}\n",
    "        \n",
    "    def preprocess_dataframe(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean and preprocess the input dataframe.\n",
    "        \n",
    "        Args:\n",
    "            df (pl.DataFrame): Input Polars DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            pl.DataFrame: Processed DataFrame\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting dataframe preprocessing\")\n",
    "        \n",
    "        try:\n",
    "            # Create a copy to avoid modifying original\n",
    "            processed = df.clone()\n",
    "            \n",
    "            # Clean text fields\n",
    "            self.logger.info(\"Cleaning text fields...\")\n",
    "            for col in tqdm(self.text_columns, desc=\"Processing text columns\"):\n",
    "                processed = processed.with_columns([\n",
    "                    pl.col(col).str.strip_chars()\n",
    "                    .str.replace_all(r'\\s+', ' ')  # Replace multiple spaces\n",
    "                    .str.to_lowercase()\n",
    "                    .alias(col)\n",
    "                ])\n",
    "            \n",
    "            # Clean categorical fields\n",
    "            self.logger.info(\"Cleaning categorical fields...\")\n",
    "            for col in tqdm(self.categorical_columns, desc=\"Processing categorical columns\"):\n",
    "                processed = processed.with_columns([\n",
    "                    pl.col(col).str.strip_chars()\n",
    "                    .str.to_lowercase()\n",
    "                    .alias(col)\n",
    "                ])\n",
    "            \n",
    "            # Check if Date needs conversion\n",
    "            if df.schema['Date'] == pl.Date:\n",
    "                self.logger.info(\"Date column already in correct format\")\n",
    "                processed = processed\n",
    "            else:\n",
    "                self.logger.info(\"Converting Date column to datetime format\")\n",
    "                processed = processed.with_columns([\n",
    "                    pl.col('Date').str.strptime(pl.Datetime, '%Y-%m-%d').alias('Date')\n",
    "                ])\n",
    "            \n",
    "            # Add derived features from date\n",
    "            processed = processed.with_columns([\n",
    "                pl.col('Date').cast(pl.Date).dt.year().alias('Year'),\n",
    "                pl.col('Date').cast(pl.Date).dt.month().alias('Month')\n",
    "            ])\n",
    "            \n",
    "            self.processed_df = processed\n",
    "            self.logger.info(\"Dataframe preprocessing completed successfully\")\n",
    "            return processed\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in preprocessing: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of texts to encode\n",
    "            batch_size (int): Batch size for encoding\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Array of embeddings\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Generating embeddings for {len(texts)} texts\")\n",
    "        \n",
    "        try:\n",
    "            # Check cache first\n",
    "            cached_embeddings = []\n",
    "            texts_to_encode = []\n",
    "            \n",
    "            for text in texts:\n",
    "                if text in self.embeddings_cache:\n",
    "                    cached_embeddings.append(self.embeddings_cache[text])\n",
    "                else:\n",
    "                    texts_to_encode.append(text)\n",
    "            \n",
    "            # Generate new embeddings\n",
    "            if texts_to_encode:\n",
    "                self.logger.info(f\"Generating new embeddings for {len(texts_to_encode)} texts\")\n",
    "                new_embeddings = self.text_encoder.encode(\n",
    "                    texts_to_encode,\n",
    "                    batch_size=self.batch_size,\n",
    "                    show_progress_bar=True\n",
    "                )\n",
    "                \n",
    "                # Update cache\n",
    "                for text, embedding in zip(texts_to_encode, new_embeddings):\n",
    "                    self.embeddings_cache[text] = embedding\n",
    "                    cached_embeddings.append(embedding)\n",
    "            \n",
    "            return np.array(cached_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating embeddings: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def create_categorical_features(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create one-hot encoded features for categorical columns.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: Dictionary of categorical features\n",
    "        \"\"\"\n",
    "        if self.processed_df is None:\n",
    "            raise ValueError(\"Please run preprocess_dataframe first\")\n",
    "            \n",
    "        self.logger.info(\"Creating categorical features\")\n",
    "        \n",
    "        try:\n",
    "            categorical_features = {}\n",
    "            \n",
    "            self.logger.info(\"Creating one-hot encodings for categorical features...\")\n",
    "            for col in tqdm(self.categorical_columns, desc=\"Processing categorical features\"):\n",
    "                # Get unique values\n",
    "                unique_values = self.processed_df[col].unique().to_list()\n",
    "                \n",
    "                # Create mapping\n",
    "                value_to_idx = {val: idx for idx, val in enumerate(unique_values)}\n",
    "                \n",
    "                # Create one-hot encodings\n",
    "                n_values = len(unique_values)\n",
    "                n_samples = len(self.processed_df)\n",
    "                one_hot = np.zeros((n_samples, n_values))\n",
    "                \n",
    "                for idx, value in enumerate(self.processed_df[col]):\n",
    "                    one_hot[idx, value_to_idx[value]] = 1\n",
    "                    \n",
    "                categorical_features[col] = one_hot\n",
    "                \n",
    "            self.logger.info(\"Categorical features created successfully\")\n",
    "            return categorical_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating categorical features: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def create_temporal_features(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create temporal features from the Date column.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Array of temporal features\n",
    "        \"\"\"\n",
    "        if self.processed_df is None:\n",
    "            raise ValueError(\"Please run preprocess_dataframe first\")\n",
    "            \n",
    "        self.logger.info(\"Creating temporal features\")\n",
    "        \n",
    "        try:\n",
    "            # Extract year and month\n",
    "            years = self.processed_df['Year'].to_numpy()\n",
    "            months = self.processed_df['Month'].to_numpy()\n",
    "            \n",
    "            # Normalize\n",
    "            min_year = years.min()\n",
    "            years_norm = (years - min_year) / 10  # Decade scale\n",
    "            months_norm = months / 12\n",
    "            \n",
    "            # Combine features\n",
    "            temporal_features = np.column_stack([years_norm, months_norm])\n",
    "            \n",
    "            self.logger.info(\"Temporal features created successfully\")\n",
    "            return temporal_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating temporal features: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def process_all(self, df: pl.DataFrame, show_progress: bool = True) -> Dict[str, Union[pl.DataFrame, np.ndarray, Dict]]:\n",
    "        \"\"\"\n",
    "        Run all processing steps and return combined results.\n",
    "        \n",
    "        Args:\n",
    "            df (pl.DataFrame): Input DataFrame\n",
    "            show_progress (bool): Whether to show progress bars\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing processed data and features\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Run all processing steps and return combined results.\n",
    "        \n",
    "        Args:\n",
    "            df (pl.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing processed data and features\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting complete data processing pipeline\")\n",
    "        \n",
    "        try:\n",
    "            # Run all processing steps\n",
    "            processed_df = self.preprocess_dataframe(df)\n",
    "            \n",
    "            # Generate text embeddings\n",
    "            query_embeddings = self.generate_embeddings(processed_df['QueryText'].to_list())\n",
    "            answer_embeddings = self.generate_embeddings(processed_df['KccAns'].to_list())\n",
    "            \n",
    "            # Create other features\n",
    "            categorical_features = self.create_categorical_features()\n",
    "            temporal_features = self.create_temporal_features()\n",
    "            \n",
    "            results = {\n",
    "                'processed_df': processed_df,\n",
    "                'query_embeddings': query_embeddings,\n",
    "                'answer_embeddings': answer_embeddings,\n",
    "                'categorical_features': categorical_features,\n",
    "                'temporal_features': temporal_features\n",
    "            }\n",
    "            \n",
    "            self.logger.info(\"Complete processing pipeline finished successfully\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in processing pipeline: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading text encoder model: BAAI/bge-large-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n",
      "INFO:__main__:Starting complete data processing pipeline\n",
      "INFO:__main__:Starting dataframe preprocessing\n",
      "INFO:__main__:Cleaning text fields...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4603aca93f3d4783b4c33a555c64cd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing text columns:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Cleaning categorical fields...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afcb4ff35b745cdb201010c5f7d1dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing categorical columns:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Date column already in correct format\n",
      "INFO:__main__:Dataframe preprocessing completed successfully\n",
      "INFO:__main__:Generating embeddings for 10000 texts\n",
      "INFO:__main__:Generating new embeddings for 10000 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f940f37634ab4d749bc4595572c314b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating embeddings for 10000 texts\n",
      "INFO:__main__:Generating new embeddings for 9977 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e476d3f3b8a449093f50a6fb103b05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating categorical features\n",
      "INFO:__main__:Creating one-hot encodings for categorical features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464f4cc162354bffae395113252a6f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing categorical features:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Categorical features created successfully\n",
      "INFO:__main__:Creating temporal features\n",
      "INFO:__main__:Temporal features created successfully\n",
      "INFO:__main__:Complete processing pipeline finished successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed successfully\n",
      "\n",
      "Sample of processed data:\n",
      "shape: (5, 12)\n",
      "┌────────┬────────┬────────┬────────┬───────┬───────┬───────┬───────┬───────┬───────┬──────┬───────┐\n",
      "│ BlockN ┆ Crop   ┆ Distri ┆ QueryT ┆ Seaso ┆ Secto ┆ State ┆ Query ┆ KccAn ┆ Date  ┆ Year ┆ Month │\n",
      "│ ame    ┆ ---    ┆ ctName ┆ ype    ┆ n     ┆ r     ┆ Name  ┆ Text  ┆ s     ┆ ---   ┆ ---  ┆ ---   │\n",
      "│ ---    ┆ str    ┆ ---    ┆ ---    ┆ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   ┆ date  ┆ i32  ┆ i8    │\n",
      "│ str    ┆        ┆ str    ┆ str    ┆ str   ┆ str   ┆ str   ┆ str   ┆ str   ┆       ┆      ┆       │\n",
      "╞════════╪════════╪════════╪════════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪══════╪═══════╡\n",
      "│ MOHANP ┆ coconu ┆ samast ┆ fertil ┆ khari ┆ horti ┆ bihar ┆ ferti ┆ ferti ┆ 2007- ┆ 2007 ┆ 1     │\n",
      "│ UR     ┆ t      ┆ ipur   ┆ izer   ┆ f     ┆ cultu ┆       ┆ lizer ┆ lizer ┆ 01-05 ┆      ┆       │\n",
      "│        ┆        ┆        ┆ use    ┆       ┆ re    ┆       ┆ doses ┆ are   ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ and    ┆       ┆       ┆       ┆ of    ┆ npk   ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ availa ┆       ┆       ┆       ┆ cocon ┆ 1:2:2 ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ bility ┆       ┆       ┆       ┆ ut    ┆ kgpla ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ nt    ┆       ┆      ┆       │\n",
      "│ DOLONG ┆ banana ┆ nagaon ┆ fertil ┆ jayad ┆ horti ┆ assam ┆ askin ┆ sugge ┆ 2009- ┆ 2009 ┆ 9     │\n",
      "│ GHAT   ┆        ┆        ┆ izer   ┆       ┆ cultu ┆       ┆ g     ┆ sted  ┆ 09-29 ┆      ┆       │\n",
      "│        ┆        ┆        ┆ use    ┆       ┆ re    ┆       ┆ about ┆ to    ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ and    ┆       ┆       ┆       ┆ the   ┆ apply ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ availa ┆       ┆       ┆       ┆ ferti ┆ urea2 ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ bility ┆       ┆       ┆       ┆ lizer ┆ 42gra ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ sched ┆ mplan ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ ule   ┆ tssp2 ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ for   ┆ 06gra ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ banan ┆ mplan ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ a cul ┆ tmop5 ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ tivat ┆ 51gra ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ ion   ┆ mplan ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ t and ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ compo ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ st12k ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ gplan ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ t in  ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ trenc ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ h met ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ hod   ┆       ┆      ┆       │\n",
      "│ DANIYA ┆ wheat  ┆ patna  ┆ fertil ┆ khari ┆ agric ┆ bihar ┆ askin ┆ askin ┆ 2009- ┆ 2009 ┆ 12    │\n",
      "│ WAN    ┆        ┆        ┆ izer   ┆ f     ┆ ultur ┆       ┆ g     ┆ g     ┆ 12-23 ┆      ┆       │\n",
      "│        ┆        ┆        ┆ use    ┆       ┆ e     ┆       ┆ about ┆ about ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ and    ┆       ┆       ┆       ┆ ferti ┆ ferti ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ availa ┆       ┆       ┆       ┆ liser ┆ liser ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆ bility ┆       ┆       ┆       ┆ dose  ┆ dose  ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ of    ┆ of    ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ wheat ┆ wheat ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ are   ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ 120kg ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ n60kg ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ p40kg ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ khect ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ for   ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ sowin ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ g     ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆       ┆ stage ┆       ┆      ┆       │\n",
      "│ AKHORI ┆ cabbag ┆ rohtas ┆ cultur ┆ khari ┆ horti ┆ bihar ┆ early ┆ pusa  ┆ 2009- ┆ 2009 ┆ 2     │\n",
      "│ GOLA   ┆ e      ┆        ┆ al pra ┆ f     ┆ cultu ┆       ┆ culti ┆ drum  ┆ 02-22 ┆      ┆       │\n",
      "│        ┆        ┆        ┆ ctices ┆       ┆ re    ┆       ┆ var   ┆ head  ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ of    ┆       ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ cabba ┆       ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ ge    ┆       ┆       ┆      ┆       │\n",
      "│ HATHUA ┆ gladio ┆ gopalg ┆ cultur ┆ rabi  ┆ horti ┆ bihar ┆ metho ┆ answe ┆ 2009- ┆ 2009 ┆ 5     │\n",
      "│        ┆ lus    ┆ anj    ┆ al pra ┆       ┆ cultu ┆       ┆ d of  ┆ r     ┆ 05-28 ┆      ┆       │\n",
      "│        ┆        ┆        ┆ ctices ┆       ┆ re    ┆       ┆ gladi ┆ given ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ olus  ┆ in    ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ culti ┆ detai ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ vatio ┆ ls    ┆       ┆      ┆       │\n",
      "│        ┆        ┆        ┆        ┆       ┆       ┆       ┆ n     ┆       ┆       ┆      ┆       │\n",
      "└────────┴────────┴────────┴────────┴───────┴───────┴───────┴───────┴───────┴───────┴──────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "# Use your filtered DataFrame\n",
    "df = filtered_df.head(10000)\n",
    "processor = DataProcessor()\n",
    "results = processor.process_all(df)\n",
    "print(\"Processing completed successfully\")\n",
    "\n",
    "# Verify the results\n",
    "processed_df = results['processed_df']\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "class GraphBuilder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        similarity_threshold: float = 0.7,\n",
    "        max_edges_per_node: int = 5,\n",
    "        device: str = 'cuda'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the GraphBuilder.\n",
    "        \n",
    "        Args:\n",
    "            similarity_threshold: Minimum similarity score to create an edge\n",
    "            max_edges_per_node: Maximum number of edges per node\n",
    "            device: 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.max_edges_per_node = max_edges_per_node\n",
    "        self.device = device\n",
    "        self.graph = None\n",
    "        \n",
    "    def compute_text_similarity_edges(\n",
    "        self,\n",
    "        embeddings: np.ndarray,\n",
    "        batch_size: int = 128\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        Compute edges based on text embedding similarity.\n",
    "        Uses batched processing to handle large matrices.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: numpy array of text embeddings\n",
    "            batch_size: size of batches for similarity computation\n",
    "            \n",
    "        Returns:\n",
    "            List of (source, target, weight) tuples\n",
    "        \"\"\"\n",
    "        num_samples = len(embeddings)\n",
    "        edges = []\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        embeddings_tensor = torch.tensor(embeddings).to(self.device)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        embeddings_tensor = torch.nn.functional.normalize(embeddings_tensor, p=2, dim=1)\n",
    "        \n",
    "        self.logger.info(\"Computing text similarity edges...\")\n",
    "        for i in tqdm(range(0, num_samples, batch_size)):\n",
    "            batch_end = min(i + batch_size, num_samples)\n",
    "            batch = embeddings_tensor[i:batch_end]\n",
    "            \n",
    "            # Compute similarity for the batch\n",
    "            similarities = torch.mm(batch, embeddings_tensor.t())\n",
    "            \n",
    "            # Get top k similar nodes for each node in batch\n",
    "            values, indices = torch.topk(similarities, k=self.max_edges_per_node + 1)\n",
    "            \n",
    "            # Convert to CPU and numpy for processing\n",
    "            values = values.cpu().numpy()\n",
    "            indices = indices.cpu().numpy()\n",
    "            \n",
    "            # Create edges for nodes in batch\n",
    "            for idx, (node_values, node_indices) in enumerate(zip(values, indices)):\n",
    "                node_idx = i + idx\n",
    "                \n",
    "                # Skip self-loops and low similarity edges\n",
    "                for sim, target in zip(node_values, node_indices):\n",
    "                    if (sim > self.similarity_threshold and \n",
    "                        node_idx != target):\n",
    "                        edges.append((node_idx, target.item(), sim.item()))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def compute_metadata_similarity(\n",
    "        self,\n",
    "        categorical_features: Dict[str, np.ndarray],\n",
    "        temporal_features: np.ndarray\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        Compute edges based on metadata similarity.\n",
    "        \n",
    "        Args:\n",
    "            categorical_features: Dict of categorical feature matrices\n",
    "            temporal_features: Matrix of temporal features\n",
    "            \n",
    "        Returns:\n",
    "            List of (source, target, weight) tuples\n",
    "        \"\"\"\n",
    "        edges = []\n",
    "        num_samples = len(temporal_features)\n",
    "        \n",
    "        self.logger.info(\"Computing metadata similarity edges...\")\n",
    "        for feature_name, feature_matrix in tqdm(categorical_features.items(), \n",
    "                                               desc=\"Processing categorical features\"):\n",
    "            # Convert to torch tensors\n",
    "            feature_tensor = torch.tensor(feature_matrix).float().to(self.device)\n",
    "            \n",
    "            # Compute similarity\n",
    "            similarities = torch.mm(feature_tensor, feature_tensor.t())\n",
    "            \n",
    "            # Get edges where features match exactly\n",
    "            matches = torch.where(similarities > 0.9)\n",
    "            source_nodes = matches[0].cpu().numpy()\n",
    "            target_nodes = matches[1].cpu().numpy()\n",
    "            \n",
    "            # Add edges with metadata type as weight\n",
    "            for src, tgt in zip(source_nodes, target_nodes):\n",
    "                if src != tgt:\n",
    "                    edges.append((src, tgt, 0.5))  # Weight of 0.5 for metadata edges\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def build_graph(\n",
    "        self,\n",
    "        processed_data: Dict[str, Union[np.ndarray, Dict]]\n",
    "    ) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Build the knowledge graph using processed features.\n",
    "        \n",
    "        Args:\n",
    "            processed_data: Dictionary containing processed features\n",
    "            \n",
    "        Returns:\n",
    "            NetworkX graph\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting graph construction...\")\n",
    "        \n",
    "        # Initialize graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with features\n",
    "        for idx in range(len(processed_data['query_embeddings'])):\n",
    "            G.add_node(\n",
    "                idx,\n",
    "                query_embedding=processed_data['query_embeddings'][idx],\n",
    "                answer_embedding=processed_data['answer_embeddings'][idx]\n",
    "            )\n",
    "        \n",
    "        # Compute edges based on text similarity\n",
    "        text_edges = self.compute_text_similarity_edges(\n",
    "            processed_data['query_embeddings']\n",
    "        )\n",
    "        \n",
    "        # Compute edges based on metadata\n",
    "        metadata_edges = self.compute_metadata_similarity(\n",
    "            processed_data['categorical_features'],\n",
    "            processed_data['temporal_features']\n",
    "        )\n",
    "        \n",
    "        # Add all edges to graph\n",
    "        self.logger.info(\"Adding edges to graph...\")\n",
    "        G.add_weighted_edges_from(text_edges)\n",
    "        G.add_weighted_edges_from(metadata_edges)\n",
    "        \n",
    "        # Basic graph statistics\n",
    "        self.logger.info(f\"Graph constructed with {G.number_of_nodes()} nodes and \"\n",
    "                        f\"{G.number_of_edges()} edges\")\n",
    "        \n",
    "        self.graph = G\n",
    "        return G\n",
    "    \n",
    "    def get_node_neighbors(\n",
    "        self,\n",
    "        node_idx: int,\n",
    "        k: int = 5\n",
    "    ) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Get k-nearest neighbors for a node.\n",
    "        \n",
    "        Args:\n",
    "            node_idx: Index of the node\n",
    "            k: Number of neighbors to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (neighbor_idx, similarity) tuples\n",
    "        \"\"\"\n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Graph not built yet. Call build_graph first.\")\n",
    "            \n",
    "        neighbors = []\n",
    "        for neighbor in self.graph.neighbors(node_idx):\n",
    "            similarity = self.graph[node_idx][neighbor]['weight']\n",
    "            neighbors.append((neighbor, similarity))\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        neighbors.sort(key=lambda x: x[1], reverse=True)\n",
    "        return neighbors[:k]\n",
    "\n",
    "    def visualize_subgraph(\n",
    "        self,\n",
    "        center_node: int,\n",
    "        radius: int = 2\n",
    "    ) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Extract a subgraph centered around a node for visualization.\n",
    "        \n",
    "        Args:\n",
    "            center_node: Index of the central node\n",
    "            radius: Number of hops to include\n",
    "            \n",
    "        Returns:\n",
    "            NetworkX subgraph\n",
    "        \"\"\"\n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Graph not built yet. Call build_graph first.\")\n",
    "            \n",
    "        # Extract ego network\n",
    "        subgraph = nx.ego_graph(self.graph, center_node, radius=radius)\n",
    "        return subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import pandas as pd \n",
    "\n",
    "class GraphBuilder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_threshold: float = 0.85,\n",
    "        metadata_threshold: float = 0.95,\n",
    "        max_semantic_edges: int = 10,\n",
    "        max_metadata_edges: int = 5,\n",
    "        device: str = 'cuda'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the GraphBuilder.\n",
    "        \n",
    "        Args:\n",
    "            similarity_threshold: Minimum similarity score to create an edge\n",
    "            max_edges_per_node: Maximum number of edges per node\n",
    "            device: 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.semantic_threshold = semantic_threshold\n",
    "        self.metadata_threshold = metadata_threshold\n",
    "        self.max_semantic_edges = max_semantic_edges\n",
    "        self.max_metadata_edges = max_metadata_edges\n",
    "        self.device = device\n",
    "        self.graph = None\n",
    "        \n",
    "        # Feature weights for combining similarities\n",
    "        self.weights = {\n",
    "            'semantic': 0.7,    # Higher weight for semantic similarity\n",
    "            'crop': 0.1,       # Weight for same crop\n",
    "            'query_type': 0.1,  # Weight for same query type\n",
    "            'location': 0.05,   # Weight for same location\n",
    "            'season': 0.05      # Weight for same season\n",
    "        }\n",
    "        \n",
    "    def compute_text_similarity_edges(\n",
    "        self,\n",
    "        embeddings: np.ndarray,\n",
    "        batch_size: int = 128\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        Compute edges based on text embedding similarity.\n",
    "        Uses batched processing to handle large matrices.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: numpy array of text embeddings\n",
    "            batch_size: size of batches for similarity computation\n",
    "            \n",
    "        Returns:\n",
    "            List of (source, target, weight) tuples\n",
    "        \"\"\"\n",
    "        num_samples = len(embeddings)\n",
    "        edges = []\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        embeddings_tensor = torch.tensor(embeddings).to(self.device)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        embeddings_tensor = torch.nn.functional.normalize(embeddings_tensor, p=2, dim=1)\n",
    "        \n",
    "        self.logger.info(\"Computing text similarity edges...\")\n",
    "        for i in tqdm(range(0, num_samples, batch_size)):\n",
    "            batch_end = min(i + batch_size, num_samples)\n",
    "            batch = embeddings_tensor[i:batch_end]\n",
    "            \n",
    "            # Compute similarity for the batch\n",
    "            similarities = torch.mm(batch, embeddings_tensor.t())\n",
    "            \n",
    "            # Get top k similar nodes for each node in batch\n",
    "            values, indices = torch.topk(similarities, k=self.max_semantic_edges + 1)\n",
    "            \n",
    "            # Convert to CPU and numpy for processing\n",
    "            values = values.cpu().numpy()\n",
    "            indices = indices.cpu().numpy()\n",
    "            \n",
    "            # Create edges for nodes in batch\n",
    "            for idx, (node_values, node_indices) in enumerate(zip(values, indices)):\n",
    "                node_idx = i + idx\n",
    "                \n",
    "                # Skip self-loops and low similarity edges\n",
    "                for sim, target in zip(node_values, node_indices):\n",
    "                    if (sim > self.semantic_threshold and \n",
    "                        node_idx != target):\n",
    "                        edges.append((node_idx, target.item(), sim.item()))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def compute_metadata_similarity(\n",
    "        self,\n",
    "        categorical_features: Dict[str, np.ndarray],\n",
    "        temporal_features: np.ndarray,\n",
    "        processed_df: pd.DataFrame\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        Compute edges based on weighted metadata similarity.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Compute edges based on metadata similarity.\n",
    "        \n",
    "        Args:\n",
    "            categorical_features: Dict of categorical feature matrices\n",
    "            temporal_features: Matrix of temporal features\n",
    "            \n",
    "        Returns:\n",
    "            List of (source, target, weight) tuples\n",
    "        \"\"\"\n",
    "        edges = []\n",
    "        num_samples = len(temporal_features)\n",
    "        \n",
    "        self.logger.info(\"Computing metadata similarity edges...\")\n",
    "        for feature_name, feature_matrix in tqdm(categorical_features.items(), \n",
    "                                               desc=\"Processing categorical features\"):\n",
    "            # Convert to torch tensors\n",
    "            feature_tensor = torch.tensor(feature_matrix).float().to(self.device)\n",
    "            \n",
    "            # Compute similarity\n",
    "            similarities = torch.mm(feature_tensor, feature_tensor.t())\n",
    "            \n",
    "            # Calculate weighted similarity\n",
    "            if feature_name in ['Crop', 'QueryType']:\n",
    "                weight = self.weights.get(feature_name.lower(), 0.05)\n",
    "                \n",
    "                # Get top matches for each node\n",
    "                values, indices = torch.topk(similarities, k=self.max_metadata_edges + 1)\n",
    "                values = values.cpu().numpy()\n",
    "                indices = indices.cpu().numpy()\n",
    "                \n",
    "                # Add weighted edges\n",
    "                for idx, (node_values, node_indices) in enumerate(zip(values, indices)):\n",
    "                    for sim, target in zip(node_values[1:], node_indices[1:]):  # Skip self\n",
    "                        if sim > self.metadata_threshold:\n",
    "                            edges.append((idx, target, sim * weight))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def build_graph(\n",
    "        self,\n",
    "        processed_data: Dict[str, Union[np.ndarray, Dict]]\n",
    "    ) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Build the knowledge graph using processed features.\n",
    "        \n",
    "        Args:\n",
    "            processed_data: Dictionary containing processed features\n",
    "            \n",
    "        Returns:\n",
    "            NetworkX graph\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting graph construction...\")\n",
    "        \n",
    "        # Initialize graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with features\n",
    "        for idx in range(len(processed_data['query_embeddings'])):\n",
    "            G.add_node(\n",
    "                idx,\n",
    "                query_embedding=processed_data['query_embeddings'][idx],\n",
    "                answer_embedding=processed_data['answer_embeddings'][idx]\n",
    "            )\n",
    "        \n",
    "        # Compute edges based on text similarity\n",
    "        text_edges = self.compute_text_similarity_edges(\n",
    "            processed_data['query_embeddings']\n",
    "        )\n",
    "        \n",
    "        # Compute edges based on metadata\n",
    "        metadata_edges = self.compute_metadata_similarity(\n",
    "            processed_data['categorical_features'],\n",
    "            processed_data['temporal_features']\n",
    "        )\n",
    "        \n",
    "        # Add all edges to graph\n",
    "        self.logger.info(\"Adding edges to graph...\")\n",
    "        G.add_weighted_edges_from(text_edges)\n",
    "        G.add_weighted_edges_from(metadata_edges)\n",
    "        \n",
    "        # Basic graph statistics\n",
    "        self.logger.info(f\"Graph constructed with {G.number_of_nodes()} nodes and \"\n",
    "                        f\"{G.number_of_edges()} edges\")\n",
    "        \n",
    "        self.graph = G\n",
    "        return G\n",
    "    \n",
    "    def get_node_neighbors(\n",
    "        self,\n",
    "        node_idx: int,\n",
    "        k: int = 5\n",
    "    ) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Get k-nearest neighbors for a node.\n",
    "        \n",
    "        Args:\n",
    "            node_idx: Index of the node\n",
    "            k: Number of neighbors to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (neighbor_idx, similarity) tuples\n",
    "        \"\"\"\n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Graph not built yet. Call build_graph first.\")\n",
    "            \n",
    "        neighbors = []\n",
    "        for neighbor in self.graph.neighbors(node_idx):\n",
    "            similarity = self.graph[node_idx][neighbor]['weight']\n",
    "            neighbors.append((neighbor, similarity))\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        neighbors.sort(key=lambda x: x[1], reverse=True)\n",
    "        return neighbors[:k]\n",
    "\n",
    "    def visualize_subgraph(\n",
    "        self,\n",
    "        center_node: int,\n",
    "        radius: int = 2\n",
    "    ) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Extract a subgraph centered around a node for visualization.\n",
    "        \n",
    "        Args:\n",
    "            center_node: Index of the central node\n",
    "            radius: Number of hops to include\n",
    "            \n",
    "        Returns:\n",
    "            NetworkX subgraph\n",
    "        \"\"\"\n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Graph not built yet. Call build_graph first.\")\n",
    "            \n",
    "        # Extract ego network\n",
    "        subgraph = nx.ego_graph(self.graph, center_node, radius=radius)\n",
    "        return subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "class GraphBuilder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        semantic_threshold: float = 0.85,\n",
    "        metadata_threshold: float = 0.95,\n",
    "        max_semantic_edges: int = 10,\n",
    "        max_metadata_edges: int = 5,\n",
    "        device: str = 'cuda'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the GraphBuilder.\n",
    "        \n",
    "        Args:\n",
    "            similarity_threshold: Minimum similarity score to create an edge\n",
    "            max_edges_per_node: Maximum number of edges per node\n",
    "            device: 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.semantic_threshold = semantic_threshold\n",
    "        self.metadata_threshold = metadata_threshold\n",
    "        self.max_semantic_edges = max_semantic_edges\n",
    "        self.max_metadata_edges = max_metadata_edges\n",
    "        self.device = device\n",
    "        self.graph = None\n",
    "        \n",
    "        # Feature weights for combining similarities\n",
    "        self.weights = {\n",
    "            'semantic': 0.7,    # Higher weight for semantic similarity\n",
    "            'crop': 0.1,       # Weight for same crop\n",
    "            'query_type': 0.1,  # Weight for same query type\n",
    "            'location': 0.05,   # Weight for same location\n",
    "            'season': 0.05      # Weight for same season\n",
    "        }\n",
    "        \n",
    "    def compute_text_similarity_edges(\n",
    "        self,\n",
    "        embeddings: np.ndarray,\n",
    "        batch_size: int = 128\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        Compute edges based on text embedding similarity.\n",
    "        Uses batched processing to handle large matrices.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: numpy array of text embeddings\n",
    "            batch_size: size of batches for similarity computation\n",
    "            \n",
    "        Returns:\n",
    "            List of (source, target, weight) tuples\n",
    "        \"\"\"\n",
    "        num_samples = len(embeddings)\n",
    "        edges = []\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        embeddings_tensor = torch.tensor(embeddings).to(self.device)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        embeddings_tensor = torch.nn.functional.normalize(embeddings_tensor, p=2, dim=1)\n",
    "        \n",
    "        self.logger.info(\"Computing text similarity edges...\")\n",
    "        for i in tqdm(range(0, num_samples, batch_size)):\n",
    "            batch_end = min(i + batch_size, num_samples)\n",
    "            batch = embeddings_tensor[i:batch_end]\n",
    "            \n",
    "            # Compute similarity for the batch\n",
    "            similarities = torch.mm(batch, embeddings_tensor.t())\n",
    "            \n",
    "            # Get top k similar nodes for each node in batch\n",
    "            values, indices = torch.topk(similarities, k=self.max_semantic_edges + 1)\n",
    "            \n",
    "            # Convert to CPU and numpy for processing\n",
    "            values = values.cpu().numpy()\n",
    "            indices = indices.cpu().numpy()\n",
    "            \n",
    "            # Create edges for nodes in batch\n",
    "            for idx, (node_values, node_indices) in enumerate(zip(values, indices)):\n",
    "                node_idx = i + idx\n",
    "                \n",
    "                # Skip self-loops and low similarity edges\n",
    "                for sim, target in zip(node_values, node_indices):\n",
    "                    if (sim > self.semantic_threshold and \n",
    "                        node_idx != target):\n",
    "                        edges.append((node_idx, target.item(), sim.item()))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def compute_metadata_similarity(\n",
    "        self,\n",
    "        categorical_features: Dict[str, np.ndarray],\n",
    "        temporal_features: np.ndarray,\n",
    "        processed_df: pd.DataFrame\n",
    "    ) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        Compute edges based on weighted metadata similarity.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Compute edges based on metadata similarity.\n",
    "        \n",
    "        Args:\n",
    "            categorical_features: Dict of categorical feature matrices\n",
    "            temporal_features: Matrix of temporal features\n",
    "            \n",
    "        Returns:\n",
    "            List of (source, target, weight) tuples\n",
    "        \"\"\"\n",
    "        edges = []\n",
    "        num_samples = len(temporal_features)\n",
    "        \n",
    "        self.logger.info(\"Computing metadata similarity edges...\")\n",
    "        for feature_name, feature_matrix in tqdm(categorical_features.items(), \n",
    "                                               desc=\"Processing categorical features\"):\n",
    "            # Convert to torch tensors\n",
    "            feature_tensor = torch.tensor(feature_matrix).float().to(self.device)\n",
    "            \n",
    "            # Compute similarity\n",
    "            similarities = torch.mm(feature_tensor, feature_tensor.t())\n",
    "            \n",
    "            # Calculate weighted similarity\n",
    "            if feature_name in ['Crop', 'QueryType']:\n",
    "                weight = self.weights.get(feature_name.lower(), 0.05)\n",
    "                \n",
    "                # Get top matches for each node\n",
    "                values, indices = torch.topk(similarities, k=self.max_metadata_edges + 1)\n",
    "                values = values.cpu().numpy()\n",
    "                indices = indices.cpu().numpy()\n",
    "                \n",
    "                # Add weighted edges\n",
    "                for idx, (node_values, node_indices) in enumerate(zip(values, indices)):\n",
    "                    for sim, target in zip(node_values[1:], node_indices[1:]):  # Skip self\n",
    "                        if sim > self.metadata_threshold:\n",
    "                            edges.append((idx, target, sim * weight))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def build_graph(\n",
    "        self,\n",
    "        processed_data: Dict[str, Union[np.ndarray, Dict]],\n",
    "        processed_df: Union[pd.DataFrame, pl.DataFrame] = None\n",
    "    ) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Build the knowledge graph using processed features.\n",
    "        \n",
    "        Args:\n",
    "            processed_data: Dictionary containing processed features\n",
    "            processed_df: Original processed dataframe with metadata\n",
    "            \n",
    "        Returns:\n",
    "            NetworkX graph\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Build the knowledge graph using processed features.\n",
    "        \n",
    "        Args:\n",
    "            processed_data: Dictionary containing processed features\n",
    "            \n",
    "        Returns:\n",
    "            NetworkX graph\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting graph construction...\")\n",
    "        \n",
    "        # Initialize graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with features\n",
    "        for idx in range(len(processed_data['query_embeddings'])):\n",
    "            G.add_node(\n",
    "                idx,\n",
    "                query_embedding=processed_data['query_embeddings'][idx],\n",
    "                answer_embedding=processed_data['answer_embeddings'][idx]\n",
    "            )\n",
    "        \n",
    "        # Compute edges based on text similarity\n",
    "        text_edges = self.compute_text_similarity_edges(\n",
    "            processed_data['query_embeddings']\n",
    "        )\n",
    "        \n",
    "        # Compute edges based on metadata\n",
    "        metadata_edges = self.compute_metadata_similarity(\n",
    "            processed_data['categorical_features'],\n",
    "            processed_data['temporal_features'],\n",
    "            processed_df if processed_df is not None else processed_data.get('processed_df')\n",
    "        )\n",
    "        \n",
    "        # Add all edges to graph\n",
    "        self.logger.info(\"Adding edges to graph...\")\n",
    "        G.add_weighted_edges_from(text_edges)\n",
    "        G.add_weighted_edges_from(metadata_edges)\n",
    "        \n",
    "        # Basic graph statistics\n",
    "        self.logger.info(f\"Graph constructed with {G.number_of_nodes()} nodes and \"\n",
    "                        f\"{G.number_of_edges()} edges\")\n",
    "        \n",
    "        self.graph = G\n",
    "        return G\n",
    "    \n",
    "    def get_node_neighbors(\n",
    "        self,\n",
    "        node_idx: int,\n",
    "        k: int = 5\n",
    "    ) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Get k-nearest neighbors for a node.\n",
    "        \n",
    "        Args:\n",
    "            node_idx: Index of the node\n",
    "            k: Number of neighbors to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (neighbor_idx, similarity) tuples\n",
    "        \"\"\"\n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Graph not built yet. Call build_graph first.\")\n",
    "            \n",
    "        neighbors = []\n",
    "        for neighbor in self.graph.neighbors(node_idx):\n",
    "            similarity = self.graph[node_idx][neighbor]['weight']\n",
    "            neighbors.append((neighbor, similarity))\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        neighbors.sort(key=lambda x: x[1], reverse=True)\n",
    "        return neighbors[:k]\n",
    "\n",
    "    def visualize_subgraph(\n",
    "        self,\n",
    "        center_node: int,\n",
    "        radius: int = 2\n",
    "    ) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Extract a subgraph centered around a node for visualization.\n",
    "        \n",
    "        Args:\n",
    "            center_node: Index of the central node\n",
    "            radius: Number of hops to include\n",
    "            \n",
    "        Returns:\n",
    "            NetworkX subgraph\n",
    "        \"\"\"\n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Graph not built yet. Call build_graph first.\")\n",
    "            \n",
    "        # Extract ego network\n",
    "        subgraph = nx.ego_graph(self.graph, center_node, radius=radius)\n",
    "        return subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading text encoder model: BAAI/bge-large-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n",
      "INFO:__main__:Starting complete data processing pipeline\n",
      "INFO:__main__:Starting dataframe preprocessing\n",
      "INFO:__main__:Cleaning text fields...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa11c3cc6bc40b2a5f47145d4b30d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing text columns:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Cleaning categorical fields...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4345e27b6946a392273f6f0d2687d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing categorical columns:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Date column already in correct format\n",
      "INFO:__main__:Dataframe preprocessing completed successfully\n",
      "INFO:__main__:Generating embeddings for 10000 texts\n",
      "INFO:__main__:Generating new embeddings for 10000 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cfcea18a4144b9b32fc79169dba1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating embeddings for 10000 texts\n",
      "INFO:__main__:Generating new embeddings for 9977 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9dd205621148ce8a455684a827fb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating categorical features\n",
      "INFO:__main__:Creating one-hot encodings for categorical features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9c7aa7c676433884521e67f8659bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing categorical features:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Categorical features created successfully\n",
      "INFO:__main__:Creating temporal features\n",
      "INFO:__main__:Temporal features created successfully\n",
      "INFO:__main__:Complete processing pipeline finished successfully\n",
      "INFO:__main__:Starting graph construction...\n",
      "INFO:__main__:Computing text similarity edges...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1901544c8314b07b5fa4bcfc094a9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Computing metadata similarity edges...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d637438ed5746d48826b97670309faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing categorical features:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Adding edges to graph...\n",
      "INFO:__main__:Graph constructed with 10000 nodes and 158368 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph Statistics:\n",
      "Number of nodes: 10000\n",
      "Number of edges: 158368\n",
      "Average degree: 31.67\n",
      "\n",
      "Example Query and Similar Questions:\n",
      "\n",
      "Original Question: FERTILIZER DOSES OF COCONUT\n",
      "\n",
      "Similar Questions:\n",
      "Similarity: 0.100\n",
      "Question: COCONUT PRICE\n",
      "Answer: GIVEN\n",
      "\n",
      "Similarity: 0.100\n",
      "Question: COCONUT PRICE\n",
      "Answer: PRICE DETAILS GIVEN\n",
      "\n",
      "Similarity: 0.100\n",
      "Question: COCONUT MITES\n",
      "Answer: 2 ML MANOLT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import polars as pl\n",
    "# import pandas as pd\n",
    "# from data_processor import DataProcessor\n",
    "# from graph_builder import GraphBuilder\n",
    "\n",
    "# Take first 10000 rows\n",
    "df = filtered_df.head(10000)\n",
    "\n",
    "# Step 1: Process the data\n",
    "processor = DataProcessor(\n",
    "    model_name='BAAI/bge-large-en-v1.5',\n",
    "    device='cuda',\n",
    "    batch_size=128\n",
    ")\n",
    "processed_data = processor.process_all(df)\n",
    "\n",
    "# Step 2: Build the graph with stricter parameters\n",
    "builder = GraphBuilder(\n",
    "    semantic_threshold=0.85,    # High threshold for semantic similarity\n",
    "    metadata_threshold=0.95,    # Very high threshold for metadata matching\n",
    "    max_semantic_edges=10,      # Limit semantic edges\n",
    "    max_metadata_edges=5,       # Limit metadata edges\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Create the knowledge graph - now passing the processed dataframe\n",
    "G = builder.build_graph(\n",
    "    processed_data=processed_data,\n",
    "    processed_df=processed_data['processed_df']\n",
    ")\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nGraph Statistics:\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Average degree: {2*G.number_of_edges()/G.number_of_nodes():.2f}\")\n",
    "\n",
    "# Look at sample connections\n",
    "sample_node = 0\n",
    "neighbors = builder.get_node_neighbors(sample_node, k=3)\n",
    "print(f\"\\nExample Query and Similar Questions:\")\n",
    "print(f\"\\nOriginal Question: {df['QueryText'][sample_node]}\")\n",
    "print(\"\\nSimilar Questions:\")\n",
    "for neighbor_idx, similarity in neighbors:\n",
    "    print(f\"Similarity: {similarity:.3f}\")\n",
    "    print(f\"Question: {df['QueryText'][neighbor_idx]}\")\n",
    "    print(f\"Answer: {df['KccAns'][neighbor_idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
